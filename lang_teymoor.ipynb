{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNsUHREEm48TUF4hUYcJKbg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amirmohammadkalateh/lang/blob/main/lang_teymoor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N8aezAbSdmHF",
        "outputId": "66070c6d-65de-4b3f-f2b2-5aa56201bd42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/69.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.2/69.2 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m37.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q -U langchain langchain-openai langchain_community"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "api_key = userdata.get('API_KEY')"
      ],
      "metadata": {
        "id": "XP7pHDiUgugV"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(model='mistralai/mistral-small-3.2-24b-instruct-2506:free' , temperature=0, api_key=api_key, base_url=\"https://openrouter.ai/api/v1\")"
      ],
      "metadata": {
        "id": "xfzhl6mohdcr"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ryvhxrkMhyh4",
        "outputId": "ebc6adb1-642f-40e6-f08b-a9f3e04ac3ef"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7f8521cf3390>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7f8521cf1a10>, root_client=<openai.OpenAI object at 0x7f8521cf1b10>, root_async_client=<openai.AsyncOpenAI object at 0x7f8521cf2350>, model_name='mistralai/mistral-small-3.2-24b-instruct-2506:free', temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********'), openai_api_base='https://openrouter.ai/api/v1')"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm.invoke(\"who are you and what can you do for me?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Jlba8DAh_LA",
        "outputId": "b972ff26-2c90-4c2c-f454-caebfc559e7e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"I'm a large language model created by Mistral AI. I can help with a wide range of tasks, including:\\n\\n1. **Answering Questions**: I can provide information on a variety of topics, from general knowledge and science, history, and technology, and more.\\n2. **Writing Assistance**: I can help with writing essays, articles, emails, and creative writing like stories or poems.\\n3. **Coding Help**: I can assist with programming questions, debugging code, and explaining concepts in various languages like Python, JavaScript, and more.\\n4. **Learning & Education**: I can explain complex topics in a simple way, help with homework, or provide study tips.\\n5. **Productivity & Organization**: I can help with planning, creating schedules, or brainstorming ideas.\\n6. **Language Translation & Grammar**: I can translate between languages and help improve your writing in English or other languages.\\n7. **Entertainment & Fun**: I can tell jokes, suggest books/movies, or even play simple games like trivia or word puzzles.\\n\\nWhat would you like help with today? 😊\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 227, 'prompt_tokens': 14, 'total_tokens': 241, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'mistralai/mistral-small-3.2-24b-instruct:free', 'system_fingerprint': None, 'id': 'gen-1750963294-mrTu9almpYS85aayWhjP', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--9d706762-5f26-4224-9ab5-247f85cdb98d-0', usage_metadata={'input_tokens': 14, 'output_tokens': 227, 'total_tokens': 241, 'input_token_details': {}, 'output_token_details': {}})"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a world class technical documentation writer.\"),\n",
        "    (\"user\", \"{input}\")\n",
        "])"
      ],
      "metadata": {
        "id": "qPwow_7rjKNG"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = prompt | llm"
      ],
      "metadata": {
        "id": "Khw0dhxCjqbj"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain.invoke({\"input\": \"how can langchain help with developing llm application?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NXrnmIGGj6le",
        "outputId": "f6de9ec5-85a0-4de3-cada-5c78551f5b6f"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='LangChain is a powerful framework designed to assist developers in building applications powered by Large Language Models (LLMs). It provides a suite of tools and abstractions that simplify the process of integrating LLMs into various applications. Here’s how LangChain can help with developing LLM applications:\\n\\n### 1. **Modular Components**\\nLangChain offers a collection of modular components that can be easily combined to create complex applications. These components include:\\n- **Prompts**: Predefined templates for generating text.\\n- **Indexes**: Data structures for organizing and retrieving information.\\n- **Chains**: Sequences of calls to LLMs, often combining prompts and indexes.\\n- **Agents**: Autonomous entities that can use chains to perform tasks.\\n\\n### 2. **Prompt Management**\\nLangChain provides tools for managing and organizing prompts, which are crucial for guiding the behavior of LLMs. This includes:\\n- **Prompt Templates**: Reusable templates that can be customized with variables.\\n- **Prompt Chaining**: Combining multiple prompts to create more complex interactions.\\n\\n### 3. **Data Integration**\\nLangChain facilitates the integration of external data sources with LLMs. This includes:\\n- **Document Loaders**: Tools for loading documents from various sources (e.g., PDFs, web pages).\\n- **Text Splitters**: Utilities for splitting text into smaller chunks for processing.\\n- **Vector Stores**: Data structures for storing and retrieving text embeddings.\\n\\n### 4. **Chain Composition**\\nLangChain allows developers to compose chains of LLM calls to perform multi-step tasks. This includes:\\n- **Sequential Chains**: Chains where the output of one LLM call is used as the input to the next.\\n- **Parallel Chains**: Chains where multiple LLM calls are made in parallel, and their outputs are combined.\\n\\n### 5. **Agent Framework**\\nLangChain provides a framework for creating agents that can autonomously perform tasks using chains. This includes:\\n- **ReAct Framework**: A framework that combines reasoning and acting to perform tasks.\\n- **Custom Agents**: Tools for creating custom agents tailored to specific applications.\\n\\n### 6. **Memory Management**\\nLangChain offers tools for managing the state and memory of conversations and interactions with LLMs. This includes:\\n- **Conversation Memory**: Tools for storing and retrieving conversation history.\\n- **State Management**: Utilities for managing the state of interactions.\\n\\n### 7. **Evaluation and Testing**\\nLangChain provides tools for evaluating and testing LLM applications. This includes:\\n- **Unit Testing**: Tools for testing individual components.\\n- **Integration Testing**: Tools for testing the integration of multiple components.\\n- **Evaluation Metrics**: Metrics for assessing the performance of LLM applications.\\n\\n### 8. **Deployment and Scaling**\\nLangChain offers tools and best practices for deploying and scaling LLM applications. This includes:\\n- **API Integration**: Tools for integrating LLM applications with APIs.\\n- **Scalability**: Best practices for scaling LLM applications to handle large volumes of requests.\\n\\n### 9. **Community and Ecosystem**\\nLangChain has a growing community and ecosystem, which includes:\\n- **Pre-built Chains and Agents**: A collection of pre-built chains and agents that can be used as-is or customized.\\n- **Community Contributions**: A vibrant community that contributes to the development and improvement of LangChain.\\n\\n### Example Use Cases\\n- **Chatbots**: Building conversational agents that can understand and respond to user queries.\\n- **Content Generation**: Generating articles, reports, and other types of content.\\n- **Data Analysis**: Analyzing and summarizing large datasets.\\n- **Automation**: Automating tasks such as email responses, customer support, and more.\\n\\nBy leveraging LangChain’s modular components, prompt management, data integration, chain composition, agent framework, memory management, evaluation and testing tools, deployment and scaling best practices, and community support, developers can efficiently build powerful and sophisticated LLM applications.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 801, 'prompt_tokens': 25, 'total_tokens': 826, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'mistralai/mistral-small-3.2-24b-instruct:free', 'system_fingerprint': None, 'id': 'gen-1750963572-HVp3TFfyYWjFNri4tDoe', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--b03c1224-3bad-41e7-a6fc-d6957210e704-0', usage_metadata={'input_tokens': 25, 'output_tokens': 801, 'total_tokens': 826, 'input_token_details': {}, 'output_token_details': {}})"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "output_parser = StrOutputParser()"
      ],
      "metadata": {
        "id": "PR_0n9K-kTKs"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = prompt | llm | output_parser"
      ],
      "metadata": {
        "id": "jPDyMd6bkqc7"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain.invoke({\"input\": \"how can langchain help with developing llm application?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 693
        },
        "id": "-dvaroo8kz6A",
        "outputId": "05e7549d-819f-4fef-c05f-0522ed06d3c2"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'LangChain is a powerful framework designed to assist developers in building applications powered by Large Language Models (LLMs). It provides a suite of tools and abstractions that simplify the process of integrating LLMs into various applications. Here’s how LangChain can help with developing LLM applications:\\n\\n### 1. **Modular Components**\\nLangChain offers a collection of modular components that can be easily combined to create complex applications. These components include:\\n- **Prompts**: Predefined templates for generating text.\\n- **Indexes**: Data structures for organizing and retrieving information.\\n- **Chains**: Sequences of calls that can be composed to perform complex tasks.\\n- **Agents**: Autonomous entities that can perform tasks by selecting and executing chains.\\n\\n### 2. **Prompt Management**\\nLangChain provides tools for managing and organizing prompts, which are crucial for guiding the behavior of LLMs. This includes:\\n- **Prompt Templates**: Reusable templates that can be customized with variables.\\n- **Prompt Chaining**: Combining multiple prompts to create more complex interactions.\\n\\n### 3. **Data Integration**\\nLangChain facilitates the integration of external data sources with LLMs. This includes:\\n- **Document Loaders**: Tools for loading documents from various sources (e.g., PDFs, web pages).\\n- **Text Splitters**: Utilities for splitting text into smaller chunks for processing.\\n- **Embeddings**: Techniques for converting text into numerical vectors for similarity search.\\n\\n### 4. **Chain Composition**\\nLangChain allows developers to compose chains of operations, enabling the creation of complex workflows. For example:\\n- **Sequential Chains**: Executing a series of steps in sequence.\\n- **Parallel Chains**: Running multiple steps in parallel.\\n- **Router Chains**: Dynamically routing inputs to different chains based on conditions.\\n\\n### 5. **Agent Framework**\\nLangChain provides an agent framework that allows developers to create autonomous agents capable of performing tasks by selecting and executing chains. This includes:\\n- **ReAct Framework**: Combining reasoning and acting to perform tasks.\\n- **Custom Agents**: Developing agents tailored to specific use cases.\\n\\n### 6. **Memory Management**\\nLangChain offers tools for managing conversational memory, enabling applications to maintain context over multiple interactions. This includes:\\n- **Conversation Buffer Memory**: Storing recent messages in a conversation.\\n- **Summary Buffer Memory**: Summarizing the conversation to reduce context length.\\n- **Vector Store Retriever Memory**: Using embeddings to retrieve relevant information from previous interactions.\\n\\n### 7. **Evaluation and Testing**\\nLangChain provides utilities for evaluating and testing LLM applications, ensuring they perform as expected. This includes:\\n- **Unit Testing**: Testing individual components.\\n- **Integration Testing**: Testing the interaction between components.\\n- **Evaluation Metrics**: Measuring the performance of the application.\\n\\n### 8. **Deployment and Scaling**\\nLangChain supports the deployment and scaling of LLM applications, making it easier to move from development to production. This includes:\\n- **API Integration**: Integrating with APIs for deployment.\\n- **Scalability**: Designing applications to handle increased load.\\n\\n### 9. **Community and Ecosystem**\\nLangChain benefits from a growing community and ecosystem, providing access to:\\n- **Pre-built Chains and Agents**: Ready-to-use components for common tasks.\\n- **Community Support**: Access to a network of developers and resources.\\n\\n### Example Use Cases\\n- **Chatbots**: Building conversational agents that can understand and respond to user queries.\\n- **Content Generation**: Automating the creation of text content, such as articles, reports, and summaries.\\n- **Data Analysis**: Extracting insights from unstructured data using LLMs.\\n- **Automation**: Automating repetitive tasks, such as data entry and customer support.\\n\\nBy leveraging LangChain’s tools and abstractions, developers can more efficiently build, test, and deploy LLM-powered applications, reducing the complexity and time required for development.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    }
  ]
}